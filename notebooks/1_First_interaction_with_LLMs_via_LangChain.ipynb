{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2NCTte9wmkHT",
        "8-mi3R-LuEa5",
        "5CBqNUL_qLlG",
        "3RBifm-DDi6T",
        "JErNbeauJjML"
      ],
      "authorship_tag": "ABX9TyMC0/2VtynIQibx8oXrKwdN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreabenevenut/LLM_workshop/blob/main/notebooks/1_First_interaction_with_LLMs_via_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OUR FIRST INTERACTION WITH LARGE LANGUAGE MODELS\n"
      ],
      "metadata": {
        "id": "2NCTte9wmkHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This notebook provides an introductory exploration of Large Language Models (LLMs), showcasing how to interact with them using specific inputs to generate text and conduct diverse language tasks.\n",
        "\n",
        "While employing a single LLM is enough for basic applications, more intricate tasks often necessitate chaining LLMs. Such chaining involves linking LLMs either with one another or with additional components.\n",
        "\n",
        "That is where the Python library [LangChain](https://www.langchain.com/) comes in handy. LangChain is an application framework designed to leverage  LLMs' power. The core of such library is the so called \"chain\". <br>\n",
        "A `chain` refers to a sequence of interconnected components designed to accomplish a particular task. This sequence dictates the flow of data and tasks within the AI system.\n",
        "\n",
        "For instance, a simple chain could comprise the following components:\n",
        "\n",
        "User Input ➡️ Large Language Model ➡️ Output Formatter ➡️ Response"
      ],
      "metadata": {
        "id": "K1qysKuJIGfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0: Set up"
      ],
      "metadata": {
        "id": "8-mi3R-LuEa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/datarootsio/LLMs_workshop.git"
      ],
      "metadata": {
        "id": "QALto4D3jtQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r \"/content/LLMs_workshop/requirements.txt\""
      ],
      "metadata": {
        "id": "2-7WKnHep9wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD91ZkHWp9wc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1: First API call to use LLMs"
      ],
      "metadata": {
        "id": "5CBqNUL_qLlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example of how to utilize the OpenAI API to ask a question with specific parameters:\n",
        "\n",
        "You have the flexibility to fine-tune your query using the following parameters:\n",
        "- **Temperature:** Adjust this float value from 0 to 1 to control the level of randomness in the Large Language Model's response. Lower values (closer to 0) ensure more deterministic responses, while higher values introduce increased variability.\n",
        "- **Model Name:** OpenAI provides a comprehensive selection of Large Language Models, each with unique characteristics and capabilities. For more details on the available models, refer to the [OpenAI Models Documentation](https://platform.openai.com/docs/models).\n"
      ],
      "metadata": {
        "id": "CmdI2L52k68W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "TfWhpnGo-T85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI()\n",
        "\n",
        "user_request = \"\"\"\n",
        "I have a cat and I would like to have a cool name for it, related to summer.\n",
        "Suggest me 5 cool names for my pet.\n",
        "\"\"\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-1106\",\n",
        "  temperature = 1,\n",
        "  messages = [{\"role\": \"user\", \"content\": user_request}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "6DljUhpUmz_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2: Prompt engineering via LangChain"
      ],
      "metadata": {
        "id": "3RBifm-DDi6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt engineering involves crafting specific input instructions for LLMs to achieve desired outputs/responses.\n",
        "This process involves fine-tuning the wording, structure, and context of the input prompt to make LLMs more accurate and useful across different tasks, like generating content, solving problems, or understanding language better.\n",
        "\n",
        "In the following example, we will see how we can define a very simple chain via LangChain. The essential components of our chain are an LLM model, a prompt with instructions and some input variables."
      ],
      "metadata": {
        "id": "HzJrWjodGO5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "pqrny29w_YdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=1, model_name=\"gpt-3.5-turbo-1106\")"
      ],
      "metadata": {
        "id": "T3NkXhpankY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_prompt = PromptTemplate(\n",
        "    input_variables = [\"animal\", \"element\", \"nicknames_numbers\"],\n",
        "    input_types={\n",
        "        \"animal\": str,\n",
        "        \"element\": str,\n",
        "        \"nicknames_numbers\": int\n",
        "        },\n",
        "    template = \"\"\"\n",
        "    I have a {animal} and I would like to have a cool name for it, related to {element}.\n",
        "    Suggest me {nicknames_numbers} cool names for my pet.\n",
        "    \"\"\"\n",
        "    )"
      ],
      "metadata": {
        "id": "YWMbUmqgDXsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pet_chain = LLMChain(llm=llm, prompt=my_prompt)"
      ],
      "metadata": {
        "id": "SF7vmJndELXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = pet_chain({\"animal\": \"dog\", \"element\": \"water\", \"nicknames_numbers\": 2})\n",
        "print(answer['text'])"
      ],
      "metadata": {
        "id": "aWyiKEAwFNGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = pet_chain({\"animal\": \"hamster\", \"element\": \"fire\", \"nicknames_numbers\": 10})\n",
        "print(answer['text'])"
      ],
      "metadata": {
        "id": "fGGiHP_wFeC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Dataroots needs you!"
      ],
      "metadata": {
        "id": "JErNbeauJjML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to help Dataroots HR department to recruit more people via social media. It would be handy to have a tool that generates Instagram or Linkedin posts where HR can tweak a few parameters to customize the post itself.\n",
        "\n",
        "Use the LangChain library, prompt engineering and templates to achieve the task.\n",
        "\n",
        "Suppose that HR would like to decide on the following parameters:\n",
        "- social_media\n",
        "- tone (e.g. formal or informal)\n",
        "- max number of words\n",
        "- position (e.g. ML Engineer, Data Engineer, Data Strategist, ...)\n",
        "\n",
        "Be creative and play around to grasp how interactions with LLMs work!"
      ],
      "metadata": {
        "id": "Ye0kahLJkSkE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KWJOxzqfNkKj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}