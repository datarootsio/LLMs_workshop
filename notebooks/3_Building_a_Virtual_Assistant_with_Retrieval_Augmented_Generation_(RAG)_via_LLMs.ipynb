{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreabenevenut/LLM_workshop/blob/main/notebooks/3_Building_a_Virtual_Assistant_with_Retrieval_Augmented_Generation_(RAG)_via_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlxG5NBhsNvs"
      },
      "source": [
        "# RETRIEVAL-AUGMENTED GENERATION (RAG)\n",
        "\n",
        "Suppose that we have a vast document collection. By extracting pertinent information, LLMs can be used to efficiently provide fast and precise answers when users inquire about these documents.\n",
        "\n",
        "LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model's cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).\n",
        "\n",
        "In the following notebook, we will go through the process of creating a virtual assistant powered by LLM. The virtual assistant will aid us in the investigation of our sources, with the possibility of having full human-like conversations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "YK0suQ6ihqH9"
      },
      "source": [
        "We will follow a step by step process:\n",
        "\n",
        ">[0: SET UP](#scrollTo=8-mi3R-LuEa5)\n",
        "\n",
        ">[1: CREATING OUR OWN FAQ CHATBOT](#scrollTo=PVg09NifuWsr)\n",
        "\n",
        ">[2: INTRODUCING MEMORY FOR A CONVERSATIONAL CHATBOT](#scrollTo=z5zlas7UdaEY)\n",
        "\n",
        ">[3: QUERYING OVER DATAROOTS WEBSITE](#scrollTo=A1JYOSYLWray)\n",
        "\n",
        ">[4: INTEGRATE LLMS WITH YOUTUBE VIDEOS](#scrollTo=tW-Xl8j-ybYr)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-mi3R-LuEa5"
      },
      "source": [
        "# 0: SET UP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QALto4D3jtQ0"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/datarootsio/LLMs_workshop.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-2m5k6k-eWz"
      },
      "outputs": [],
      "source": [
        "!pip install -r \"/content/LLM_workshop/requirements.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCGVD-Nl-D1-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfWhpnGo-T85"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.indexes.vectorstore import VectorstoreIndexCreator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVg09NifuWsr"
      },
      "source": [
        "# 1: CREATING OUR OWN FAQ CHATBOT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5ECcGRNYYTP"
      },
      "source": [
        "\n",
        "The goal is to ask a question to the chatbot relative to a specific document 'state of the union', relative to ... .\n",
        "\n",
        "How can we ask a question about a document (or multiple)?\n",
        "\n",
        "A very first idea could be the simple chain appraoch (put link to other notebook) and pass the whole document within the template prompt and ask a question. This would work if the document is small enough. However there are limitations (link) to how many tokens we can provide in the prompt of a Large Languge Model and that would not work. Moreover it would not be efficient, especially if we have a question about a specific part of the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1IjEdeF-xIK"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/LLM_workshop/data/state_of_the_union.txt\") as f:\n",
        "    state_of_the_union = f.read()\n",
        "len(state_of_the_union)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQA8fOUMw5ap"
      },
      "source": [
        "First, we could think of splitting the document into smaller chunks that are more manageable by our LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmUXHFQmrsSG"
      },
      "outputs": [],
      "source": [
        "def print_chunk(chunk_text):\n",
        "  chunk_text_reformatted = chunk_text.replace('\\n\\n', '\\n')\n",
        "  print(f\"'''\\n{chunk_text_reformatted}\\n'''\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b48cutKn_WlS"
      },
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
        "texts = text_splitter.split_text(state_of_the_union)\n",
        "print(\"Number of chunks: \", len(texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Xzik-H7sOiY"
      },
      "outputs": [],
      "source": [
        "print_chunk(texts[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbuI9v5hxEuB"
      },
      "source": [
        "Now that we have our document split into smaller pieces, we need to find a way to store the information efficiently.\n",
        "The idea is that, when we ask a question, we would like to retrieve the most relevant parts of the document that could be useful to answer the question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76a3J1_fbVoD"
      },
      "source": [
        "One option is to define a vector store (put definition) ...\n",
        "\n",
        "There are many different choices. Fow this tutorial we will go for Chroma\n",
        "In the link you can see all the other available vector stores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWqnpPNXQwLi"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_texts(texts,\n",
        "                              embeddings,\n",
        "                              metadatas=[{\"source\": f\"page {str(i)}\"} for i in range(len(texts))])\n",
        "print(f\"The vectore store contains {vectorstore._collection.count()} documents in total.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1jZJySjblfS"
      },
      "source": [
        "Now let's test our vector store against one arbitrary question. And check for the most relevant chunks.\n",
        "\n",
        "By default the top 4 most similar chunks to the question will be retrieved but we can change this parameter\n",
        "\n",
        "Explain better how chunks are retrieved based on similarity metric and embeddings. Add images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVTAaGy2s42x"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever()\n",
        "query = \"What did the president say about Justice Breyer\"\n",
        "docs = retriever.get_relevant_documents(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkxlrUUbufgq"
      },
      "outputs": [],
      "source": [
        "def show_docs(docs):\n",
        "  for doc in docs:\n",
        "    print_chunk(doc.page_content)\n",
        "    print(doc.metadata['source'])\n",
        "    print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl5QPBFPtrkE"
      },
      "outputs": [],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFJ_0Xv2txRT"
      },
      "outputs": [],
      "source": [
        "show_docs(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URX2oDCccIwg"
      },
      "source": [
        "Now it is time to define our FAQ chatbot that, based on the question, will retrieve the most relevant chunks and provides an answer. To define it we can handily use langchain chain \"load_qa_chain\" (put link)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLCTJb7qtyZG"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bd9IXU71b71"
      },
      "outputs": [],
      "source": [
        "chain = load_qa_chain(llm=OpenAI(temperature=0), chain_type=\"stuff\")\n",
        "query = \"What did the president say about Justice Breyer\"\n",
        "docs = retriever.get_relevant_documents(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j424Qes4yp7-"
      },
      "source": [
        "Let's see what is happenning under the hood.\n",
        "\n",
        "First, we can have a look at all the parameters that are passed to our chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPU5juf51rb_"
      },
      "outputs": [],
      "source": [
        "chain.dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQKF8SCJ18sV"
      },
      "source": [
        "In particular this is the template that is passed to our llm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWpAzCSm19E-"
      },
      "outputs": [],
      "source": [
        "print(chain.dict()['llm_chain']['prompt']['template'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwpMMa-w2G0K"
      },
      "source": [
        "We can see that the function uses two special variables:\n",
        "\n",
        "- __context__: this variable is replaced by the most relevant pieces of documents fetched from the vector store to answer the question\n",
        "\n",
        "- __question__: the question provided by the user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry-9Bmwp1xK1"
      },
      "source": [
        "Now let's rerun the chain and set `verbose=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sq-bDspy6j3"
      },
      "outputs": [],
      "source": [
        "chain = load_qa_chain(llm=OpenAI(temperature=0), chain_type=\"stuff\", verbose=True)\n",
        "query = \"What did the president say about Justice Breyer\"\n",
        "docs = retriever.get_relevant_documents(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWIj0RQLcirS"
      },
      "source": [
        "It is also possible to further customize the chain and provide our own instructions in the template. We should keep the special variables __context__ and __question__ but we can add our own variables too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUj6pq601pVk"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer in {language}:\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\", \"language\"]\n",
        ")\n",
        "chain = load_qa_chain(llm=OpenAI(temperature=0), chain_type=\"stuff\", prompt=PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti8DMWvZ1tqe"
      },
      "outputs": [],
      "source": [
        "query = \"What did the president say about Justice Breyer\"\n",
        "docs = retriever.get_relevant_documents(query)\n",
        "final_chain = chain({\"input_documents\": docs, \"question\": query, \"language\": \"Dutch\"})\n",
        "answer = final_chain['output_text']\n",
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auCgbxVF6K6V"
      },
      "outputs": [],
      "source": [
        "query = \"What did the president say about Justice Breyer\"\n",
        "docs = retriever.get_relevant_documents(query)\n",
        "final_chain = chain({\"input_documents\": docs, \"question\": query, \"language\": \"Italian\"})\n",
        "answer = final_chain['output_text']\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-EZ8hfIdvzK"
      },
      "source": [
        "GUARDRAILS?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5zlas7UdaEY"
      },
      "source": [
        "# 2: INTRODUCING MEMORY FOR A CONVERSATIONAL CHATBOT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8suNt63ddgv"
      },
      "source": [
        "A step further in the improvement of our chatbot would be to add a memory element to it so that can really feel like having a conversation with a virtual assistant rather than a simple faq engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKchjQtxdxYQ"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUqXJ1U_eUr7"
      },
      "outputs": [],
      "source": [
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0),\n",
        "    retriever=retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvEJDNgu8SvQ"
      },
      "outputs": [],
      "source": [
        "question = \"What did the president say about Justice Breyer\"\n",
        "result = qa_chain({\"query\": question})\n",
        "print(result[\"result\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOzfgM_78Tep"
      },
      "outputs": [],
      "source": [
        "question = \"Translate the previous answer in French\"\n",
        "result = qa_chain({\"query\": question})\n",
        "print(result[\"result\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Mmy1xDC9EzN"
      },
      "source": [
        "We are not keeping track of past conversations. In other words, we are not using memory. In order to create a conversational chatbot, we should include that element."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR2mXNvj8iLP"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "qa_chain_with_memory = ConversationalRetrievalChain.from_llm(\n",
        "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0),\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9wdZw63-R6j"
      },
      "outputs": [],
      "source": [
        "question = \"What did the president say about Justice Breyer?\"\n",
        "result = qa_chain_with_memory({\"question\": question})\n",
        "result['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU8VO5Dl-Tih"
      },
      "outputs": [],
      "source": [
        "question = \"Translate the previous answer in French\"\n",
        "result = qa_chain_with_memory({\"question\": question})\n",
        "result['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWALQucm-Vhv"
      },
      "outputs": [],
      "source": [
        "qa_chain_with_memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1JYOSYLWray"
      },
      "source": [
        "# 3: QUERYING OVER DATAROOTS WEBSITE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-cNJ-RBnIX3"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader([\"https://dataroots.io/our-dna\", \"https://dataroots.io/blog/state-of-data-quality-october-2023\"])\n",
        "\n",
        "dataroots_website = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZcq27oIqbtV"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
        "\n",
        "#print(\"Number of chunks: \", len(texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFYHci3JoHri"
      },
      "outputs": [],
      "source": [
        "metadatas = []\n",
        "content = []\n",
        "for web_page in dataroots_website:\n",
        "  texts = text_splitter.split_text(web_page.page_content)\n",
        "  content += texts\n",
        "  for i, text in enumerate(texts):\n",
        "    metadata = web_page.metadata.copy()\n",
        "    metadata['part'] = i + 1\n",
        "    metadatas.append(metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnGptNeKxwBD"
      },
      "outputs": [],
      "source": [
        "metadatas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMcz9S_v-ZOx"
      },
      "outputs": [],
      "source": [
        "vectorstore.delete_collection()\n",
        "embeddings = OpenAIEmbeddings()\n",
        "docs = text_splitter.create_documents(content, metadatas)\n",
        "vectorstore = Chroma.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMg5n8iPTRlW"
      },
      "outputs": [],
      "source": [
        "vectorstore._collection.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOpoZPFaoEP2"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "user_input = \"What is Dataroots DNA?\"\n",
        "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=OpenAI(temperature=0),\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")\n",
        "result = qa_chain({\"question\": user_input})\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDje8441pciN"
      },
      "outputs": [],
      "source": [
        "user_input = \"What is data quality?\"\n",
        "result = qa_chain({\"question\": user_input})\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyBxYyKnvTx8"
      },
      "outputs": [],
      "source": [
        "user_input = \"What are the most used data quality tools?\"\n",
        "result = qa_chain({\"question\": user_input})\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW-Xl8j-ybYr"
      },
      "source": [
        "# 4: INTEGRATE LLMS WITH YOUTUBE VIDEOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zocLejViyrlt"
      },
      "outputs": [],
      "source": [
        "!pip install youtube-transcript-api pytube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7DBlmc6VwpO"
      },
      "outputs": [],
      "source": [
        "docs.metadata['soruce']= \"https://www.youtube.com/watch?v=\" + docs.metadata['soruce']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWnVpjG-yjyK"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import YoutubeLoader\n",
        "\n",
        "loader = YoutubeLoader.from_youtube_url(\n",
        "    'https://www.youtube.com/watch?v=aywZrzNaKjs', add_video_info=True\n",
        ")\n",
        "\n",
        "docs = loader.load()\n",
        "docs[0].metadata['source']= \"https://www.youtube.com/watch?v=\" + docs[0].metadata['source']\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore.delete_collection()\n",
        "vectorstore = Chroma.from_documents(docs, embeddings)\n",
        "\n",
        "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=OpenAI(temperature=0),\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--rFSLIcynov"
      },
      "outputs": [],
      "source": [
        "user_input = \"What is the video about?\"\n",
        "result = qa_chain({\"question\": user_input})\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ry6Nr2nd0DJw"
      },
      "outputs": [],
      "source": [
        "user_input = \"What are the advantages of using LangChain?\"\n",
        "result = qa_chain({\"question\": user_input})\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9Q7orUjUlmz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO7mdPpaedGxjBMyCQn35JA",
      "collapsed_sections": [
        "8-mi3R-LuEa5",
        "PVg09NifuWsr",
        "z5zlas7UdaEY",
        "A1JYOSYLWray",
        "tW-Xl8j-ybYr"
      ],
      "include_colab_link": true,
      "mount_file_id": "1q2JvqY1SGC0GmIEHPUw1dyTlojTF3LfQ",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
